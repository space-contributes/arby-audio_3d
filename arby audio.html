<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Arby Audio - 3D Spatial Audio + CV (multi-object)</title>
      <meta name="description" content="Arby Audio 3D  Cinematic Spatial Sound Engine   Experience living sound that adapts bounces and reacts without configuration  with advanced objectbased 714 spatial realismMade with  by Spacecode WITH 7 YEARS OF MAKINGVersionhttpsimgshieldsiobadgeversion22blueContributions WelcomehttpsimgshieldsiobadgecontributionswelcomebrightgreenPython Versionhttpsimgshieldsiobadgepython311blueBuild StatushttpsimgshieldsiobadgebuildpassingbrightgreenLast Commithttpsimgshieldsiogithublastcommitspacecontributesarbyaudio_3dStarshttpsimgshieldsiogithuHere‚Äôs your cleaned and compacted version of the **Arby Audio 3D** README with **all random line breaks and extra spaces removed** ‚Äî fully formatted and consistent for GitHub Markdown rendering:---# Arby Audio 3D ‚Äî Cinematic Spatial Sound Engine### **Live 96 kHz / 32-bit Spatial Audio Conversion. GPU-Accelerated. Physically Accurate.**##### **Arby Audio 3D** is a **next-generation, GPU-accelerated spatial sound engine** that delivers **live 96 kHz / 32-bit, ISO 9613-1‚Äìcompliant, HRTF-accurate, multi-threaded, and privacy-safe 7.1.4 audio conversion** with **real-time reflections, sinc resampling, furniture-aware acoustics, and cross-platform AR/VR-ready performance** ‚Äî all **optimized at the assembly level for true cinematic realism.***Made with ‚ù§Ô∏è by Space-code* WITH *7 YEARS OF MAKING*![Version](https://img.shields.io/badge/version-3.2.1-blue) ![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen) ![Python Version](https://img.shields.io/badge/python-3.11-blue) ![Build Status](https://img.shields.io/badge/build-passing-brightgreen) ![Last Commit](https://img.shields.io/github/last-commit/space-contributes/arby-audio_3d) ![Stars](https://img.shields.io/github/stars/space-contributes/arby-audio_3d?style=social) ![Forks](https://img.shields.io/github/forks/space-contributes/arby-audio_3d?style=social) ![Open Issues](https://img.shields.io/github/issues/space-contributes/arby-audio_3d) ![Closed Issues](https://img.shields.io/github/issues-closed/space-contributes/arby-audio_3d) ![Downloads](https://img.shields.io/github/downloads/space-contributes/arby-audio_3d/total) ![Maintenance](https://img.shields.io/badge/maintenance-active-brightgreen) ![Supported OS](https://img.shields.io/badge/os-windows%20|%20macOS%20|%20Linux-lightgrey) ![Top Language](https://img.shields.io/github/languages/top/space-contributes/arby-audio_3d) ![Repo Size](https://img.shields.io/github/repo-size/space-contributes/arby-audio_3d) ![Commits](https://img.shields.io/github/commit-activity/m/space-contributes/arby-audio_3d) ![Issues Closed](https://img.shields.io/github/issues-pr-closed/space-contributes/arby-audio_3d)![Arby Audio Logo](https://raw.githubusercontent.com/space-contributes/arby-audio_3d/refs/heads/main/Arby%20Logo%20Design%20Proto.1\(1\).jpg)Arby Audio is a next-generation 3D spatial sound engine designed for **live, real-time, and file-based audio and video conversion**. It's engineered from the ground up for **precision, performance, and realism** ‚Äî using **sinc resampling, ISO 9613-1‚Äìcompliant attenuation**, and **HRTF-based spatial rendering** that simulates how sound truly behaves in the real world.And yes ‚Äî it sounds **AMAZING.**---## Setup & Usage### Download the ExecutableYou can **download the latest release** (recommended) ‚Äî or clone the repository manually.```bashgit clone https://github.com/space-code/arby-audio_3d.gitcd arby-audio_3d```Or download the files individually from GitHub.### ü™ü Windows Live Audio ConversionThe Windows `.exe` allows you to **convert live audio** directly into Arby Audio's spatial format.> ‚ö†Ô∏è Make sure your playback device is set to **96 kHz, 24-bit (or 32-bit if supported)** in your Windows Sound Control Panel.> Place all `.dll` files in the same folder as the `.exe`.### üêç Python VersionRun the Python version to process an audio file:```bashpython "arby_audio.py" --music_url https://your-music-url.com/file.wav```### üåê HTML / Web VersionOpen the HTML file in your browser ‚Äî or visit the hosted version on GitHub Pages.* Works **offline*** Compatible with **Windows, macOS, Linux, Android, and iOS*** 100% **local processing**, **no servers**, and **GDPR compliant**---## üß© Features* **Sinc-based resampling** up to 96 kHz / 32-bit for unmatched clarity.* **HRTF spatialization** with full 360¬∞ azimuth and elevation coverage.* **Real-time reflections** up to 3rd order with per-wall frequency damping.* **Distance- and frequency-dependent air absorption** (ISO 9613-1).* **Automatic object hardness and absorption adaptation.*** **Furniture detection** for improved realism ‚Äî no sensors required.* **Multi-threaded (up to 8 workers)** for parallel sound reflections.* **GPU-mapped memory access** for maximum efficiency.* **Multi-platform support** (Windows, macOS, Linux; live conversion in progress for macOS/Linux).* **No SDKs, no dependencies ‚Äî just pure performance.*** **AR/VR compatibility and game-ready architecture.**---## üí• Why Arby Audio Is Better Than Typical Audio EnginesMost audio frameworks rely on middleware layers, SDKs, and abstraction ‚Äî which introduce **latency**, **limited control**, and **non-optimized paths**. Arby Audio takes a completely different approach: **pure, physics-accurate sound processing**, coded directly at the **assembly level** for ultimate performance and realism.### üöÄ Performance* **Every instruction hand-optimized in pure assembly**, outperforming traditional compiled languages.* **Direct GPU-mapped memory access** ‚Äî zero driver overhead, no context switching.* **8-thread worker pool** for real-time reflection modeling and sinc-based upsampling at 96 kHz / 32-bit precision.### üéß Acoustic Realism* **Fully compliant with ISO 9613-1** for air absorption and distance-based attenuation.* **True HRTF-based spatial simulation** with full azimuth and elevation coverage (complete 360¬∞ + vertical).* **3rd-order reflection modeling**, dynamic room scaling, and frequency-dependent energy loss simulation.* **Automatic object hardness and absorption detection** ‚Äî surfaces react naturally to sound.* **Furniture detection and adaptive reflection logic** ‚Äî optimized for realistic spaces without sensors.### üåç Multi-Platform & Privacy-First* Runs fully **offline** on **Windows, macOS, and Linux**, with web and mobile support via a standalone HTML engine.* **No SDKs, no telemetry, no servers.*** 100% **privacy- and compliance-safe**, with all processing handled locally.* **Web version** uses on-device compute through WebAssembly and WebAudio for real-time rendering.### üéÆ Developer-Friendly* **Game-ready and VR/AR compatible** with parallel EXE support for multi-instance workflows.* **Drop-in executable** for live conversion ‚Äî no setup, no integration overhead.* **Automatic resampling support** from 44.1 kHz to 96 kHz, with sinc-based filtering and FFT spectral smoothing.* **Multi-channel 7.1.4 layout compatible** ‚Äî with *true* 360¬∞ HRTF spatialization.### üß† Why It Sounds Better* Typical engines approximate reflections; Arby Audio **physically simulates** them.* Typical engines use linear filters; Arby Audio applies **multi-band sinc resampling** and **FFT spectral weighting**.* Typical engines pre-render effects; Arby Audio performs **live time-domain convolution** with frequency-dependent delay and attenuation.* And most importantly ‚Äî it simply sounds **incredible** üîä---## üß™ Development Status‚úÖ Audio/Video Conversion ‚Äî Completed‚úÖ 96 kHz / 32-bit Sinc Resampling ‚Äî Completed‚úÖ ISO 9613-1 Attenuation and Frequency Loss ‚Äî Completed‚úÖ Multi-threaded Reflection Engine ‚Äî Completed‚úÖ GPU Direct Optimization ‚Äî CompletedüîÑ Linux/macOS Live Conversion ‚Äî In ProgressüîÑ AR/VR SDK Support ‚Äî PlannedüîÑ Real-time Object Hardness Toggle ‚Äî In Progress---## üîê Architecture Overview* Written in **modern C++**, with **line-by-line assembly optimization**.* Combines compiler optimizations from multiple sources and custom assembly inspection for maximum efficiency.* **Dual-compatible techniques** ensure stability on both new and older systems.* **Memory-safe** and **multi-threaded** by design.---## üìú LicenseOpen-source. Free to use, modify, and redistribute. All code executes locally and respects user privacy.LICENSE.md only valid in Main Branch.All references to brand names and trademarks are for educational and research purposes only.---Would you like me to make it **GitHub README-optimized** (with table of contents, collapsible sections, and emoji alignment)? It‚Äôll make it visually cleaner and more professional.bstarsspacecontributesarbyaudio_3dstylesocialForkshttpsimgshieldsiogithubforksspacecontributesarbyaudio_3dstylesocialOpen Issueshttpsimgshieldsiogithubissuesspacecontributesarbyaudio_3dClosed Issueshttpsimgshieldsiogithubissuesclosedspacecontributesarbyaudio_3dDownloadshttpsimgshieldsiogithubdownloadsspacecontributesarbyaudio_3dtotalMaintenancehttpsimgshieldsiobadgemaintenanceactivebrightgreenSupported OShttpsimgshieldsiobadgeoswindows2020macOS2020LinuxlightgreyTop Languagehttpsimgshieldsiogithublanguagestopspacecontributesarbyaudio_3dRepo Sizehttpsimgshieldsiogithubreposizespacecontributesarbyaudio_3dCommitshttpsimgshieldsiogithubcommitactivitymspacecontributesarbyaudio_3dIssues Closedhttpsimgshieldsiogithubissuesprclosedspacecontributesarbyaudio_3dArby Audio Logohttpsrawgithubusercontentcomspacecontributesarbyaudio_3drefsheadsmainArby20Logo20Design20Proto11jpgArby Audio delivers cinematicgrade 3D sound with immersive 714 spatial audioEnjoy living sound that reacts in real time bringing games movies and music to life with natural reflections precise positioning and stunning binaural effects  Arby Audio vs the typical system  The typical system Maps sound objects as virtual waves with delays and reflections Provides 360 surround immersion Default quality 42kHz  24bit Adaptive spatial realism across devices Widely adopted in theaters and consumer hardware depends on system configuration  Arby AudioLiving sound that bounces adapts and reacts Not just heard but felt Smart room scaling brings audio to life  Reflections furniture occlusion immersive realismwithout any sensors cameras microphones Mind  blown Arby Audio pushes beyond traditional audio engines with realworld acoustic simulation Room geometry  reflections Distancebased and sound wave bouncing time delays Frequencydependent lowpass filtering if bounced sound wave if bounced less frequency  clipping for random high frequency audio Background noise reduction Speaker mapping to 714 layout 7 down 4 up 1 sub Binaural downmix for headphones Trajectorybased moving sound sources Furnitureenvironment scanning for realistic reflections Automatic normalization  highfrequency smoothing Highfidelity output 96kHz  32bit 4 industry standard Fully open source  customizable  Feature Breakdown Room Geometry  Reflections  Sound bounces naturally off virtual walls ceilings and objects Virtual Object Detection  Sounds interact with detected scene objects Realistic Delays  Travel  reflection delays modeled after real physics MaterialAware Filtering  Simulates absorption  air damping Speaker Mapping  True 714 Atmosstyle layout Binaural Downmix  Immersive stereo playback Background Noise Removal  Removes background noise Dynamic Trajectories  Moving sources with path realism Environment Scanning  Furnitureobjects intelligently shape reflections Clipping Protection  Autonormalization ensures stable output Room Geometry  Reflections  Sound bounces naturally off virtual walls ceilings and objects Detects Virtual Objects  Sound bounces off naturally over virtual objects detected in the scene Sound Bounces  with a delay for realism to reach the object and bounce off it DistanceBased Time Delays  Delays replicate realworld propagation for precise spatialization FrequencyDependent LowPass Filtering  Simulates material absorption and air damping StudioGrade Fidelity  96kHz  32bit audio 4 the industry standard  Who Is It For  Gamers  Game Developers  Audiophiles  Music Producers  VR  AR Developers  Film  Multimedia Editors  Educational  Research Labs  Setup  UsageClone or download the repo or download the releases the latest onebashgit clone httpsgithubcomspacecodearbyaudio_3dgitcd arbyaudio_3dOR DOWNLOAD FIELS INDUVISUALLY PYTHON OR HTML YOU CHOOSEHTMLOpen the file in a browserPythonRun the engine with your audio filebashpython PYTHONSCRIPTpy music_url httpsyourmusicurlcomfilewavOR DOWNLOAD THE FILES INDIVIDUALLY through the GitHub websitereplace with your own wav URL OR FOR WINDOWSStill requires cloning the repo or downloading files individually from the GitHub websiteRun the WOOBAT as administratorThen if it gets stuck restart itIf it is not processing the WAV try shortening the URL using tinyurlcom and try again with the updated link FOR MACOS AND LINUXor WOOsh for Linux OR MacOS file as rootsudosuFor shchmod x WOOsh  sudo WOOshThen if it gets stuck restart it If it were to be stuck on Python installation confirm Python is installed by a command and then restart WOOsh because sometimes it forgets that Python is done installing  Roadmap Google Drive integration Windows bat launcher doubleclick ready GUI frontend for nontechnical users Expanded VRAR SDK support  License  LegalBy downloading installing or using Arby Audio 3D you agree to the terms in LICENSEmdLICENSEmdThis project is for educational and ethical use only For educational and ethical testing only  unauthorized use is illegal Contributions welcome Fork the repo create a branch and submit a PR Disclaimer  Educational and Ethical Use OnlyThis project is created strictly for educational and ethical use only All product names trademarks and registered trademarks mentioned are the property of their respective ownersThis project is not affiliated with endorsed by or sponsored by any company brand or trademark holderThis service is provided on a asis basis with good faith and no obligations or warranties towards the same Not to DefameThis material is intended for informational research and educational purposes only It is not intended to disparage defame or negatively impact the reputation of any company brand or trademark holderThe authors intent is strictly educational and researchfocused Any misuse of this project or its materials is the sole responsibility of the user The author shall not be liable or responsible for such misuse as that was never the intent Independent DevelopmentArby Audio 3D is an independent opensource project While it draws inspiration from cinematicgrade audio technologies such as objectbased surround and spatial audio systems it has no official affiliation with any company brand or trademark holder Trademark NoticeAll names logos and brands mentioned in this project are the property of their respective owners References are made solely for descriptive educational and comparative purposes and even indirect references and other types of references that may cause INCIDENTAL SPECIAL CONSEQUENTIAL OR PUNITIVE DAMAGES INCLUDING WITHOUT LIMITATION regarding LOSS OF PROFITS LOSS OF DATA BUSINESS INTERRUPTION OR LOSS OF BUSINESS OPPORTUNITIES"> <meta name="keywords" content="3D Audio, Surround Sound, Spatial Engine, Arby Audio, 7.1.4, Binaural"> <meta name="description" content="3D Arby Audio Engine - Realistic 7.1.4 surround sound simulation with physical reflection modeling."> <!-- Core metadata -->

  <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            min-height: 100vh;
            padding: 20px;
            color: #fff;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
        }

        header {
            text-align: center;
            margin-bottom: 40px;
        }

        .brand-logo {
            font-size: 3.5em;
            font-weight: 900;
            background: linear-gradient(135deg, #e94560, #f39c12, #e94560);
            background-size: 200% auto;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            animation: gradient 3s ease infinite;
            letter-spacing: -2px;
        }

        @keyframes gradient {
            0%, 100% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
        }

        .brand-tagline {
            font-size: 1.1em;
            opacity: 0.9;
            margin-top: 10px;
            color: #f39c12;
        }

        .main-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }

        .panel {
            background: rgba(255, 255, 255, 0.05);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 25px;
            border: 1px solid rgba(233, 69, 96, 0.3);
        }

        .panel h2 {
            font-size: 1.5em;
            margin-bottom: 20px;
            color: #e94560;
        }

        .upload-zone {
            border: 3px dashed rgba(233, 69, 96, 0.4);
            border-radius: 10px;
            padding: 40px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s;
            margin-bottom: 20px;
            background: rgba(233, 69, 96, 0.05);
        }

        .upload-zone:hover {
            border-color: #e94560;
            background: rgba(233, 69, 96, 0.1);
        }

        input[type="file"] {
            display: none;
        }

        .control-group {
            margin-bottom: 20px;
        }

        .control-group label {
            display: block;
            margin-bottom: 8px;
            font-weight: 500;
            color: #f39c12;
        }
       
        #progressBarFill {
    height: 100%;
    background: linear-gradient(90deg, #e94560, #f39c12);
    width: 0%;
    transition: width 0.3s;
    text-align: center;
    color: white;
    font-size: 0.8em;
}

        .input-row {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        input[type="range"] {
            flex: 1;
            height: 6px;
            border-radius: 3px;
            background: rgba(233, 69, 96, 0.3);
            outline: none;
            -webkit-appearance: none;
        }

        input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: linear-gradient(135deg, #e94560, #f39c12);
            cursor: pointer;
        }

        input[type="number"] {
            width: 100px;
            padding: 8px;
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(233, 69, 96, 0.3);
            border-radius: 5px;
            color: white;
            font-size: 0.9em;
            text-align: center;
        }

        .value-display {
            display: inline-block;
            background: rgba(233, 69, 96, 0.2);
            padding: 6px 12px;
            border-radius: 5px;
            font-size: 0.9em;
            min-width: 80px;
            text-align: center;
        }

        .auto-badge {
            display: inline-block;
            background: linear-gradient(135deg, #f39c12, #e67e22);
            color: white;
            padding: 3px 10px;
            border-radius: 12px;
            font-size: 0.7em;
            font-weight: 700;
            margin-left: 8px;
        }

        button {
            background: linear-gradient(135deg, #e94560 0%, #f39c12 100%);
            color: white;
            border: none;
            padding: 12px 30px;
            border-radius: 8px;
            font-size: 1em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            margin-right: 10px;
            margin-bottom: 10px;
        }

        button:hover:not(:disabled) {
            transform: translateY(-2px);
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .checkbox-group {
            margin: 15px 0;
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 10px;
            background: rgba(233, 69, 96, 0.05);
            border-radius: 8px;
        }

        input[type="checkbox"] {
            width: 20px;
            height: 20px;
            cursor: pointer;
            accent-color: #e94560;
        }

        select {
            background: rgba(233, 69, 96, 0.2);
            border: 1px solid rgba(233, 69, 96, 0.3);
            color: white;
            padding: 8px 12px;
            border-radius: 5px;
            cursor: pointer;
            outline: none;
        }

        select option {
            background: #16213e;
        }

        .info, .success, .rate-limit-info {
            padding: 12px;
            border-radius: 5px;
            margin-top: 15px;
            border-left: 4px solid;
        }

        .info {
            background: rgba(52, 152, 219, 0.2);
            border-color: #3498db;
        }

        .success {
            background: rgba(46, 204, 113, 0.2);
            border-color: #2ecc71;
        }

        .rate-limit-info {
            background: rgba(243, 156, 18, 0.2);
            border-color: #f39c12;
        }

        .progress {
            height: 6px;
            background: rgba(233, 69, 96, 0.2);
            border-radius: 3px;
            overflow: hidden;
            margin-top: 10px;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #e94560, #f39c12);
            width: 0%;
            transition: width 0.3s;
        }

        .status {
            background: rgba(0, 0, 0, 0.5);
            padding: 15px;
            border-radius: 8px;
            margin-top: 15px;
            font-family: monospace;
            font-size: 0.9em;
        }

        .status-item {
            margin: 5px 0;
            display: flex;
            justify-content: space-between;
        }

        .visualizer {
            background: rgba(0, 0, 0, 0.5);
            border-radius: 10px;
            height: 200px;
            margin-top: 20px;
        }

        canvas {
            width: 100%;
            height: 100%;
        }

        .speaker-map {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 10px;
            margin-top: 15px;
        }

        .speaker {
            background: rgba(233, 69, 96, 0.1);
            padding: 10px;
            border-radius: 8px;
            text-align: center;
            font-size: 0.85em;
            border: 2px solid rgba(233, 69, 96, 0.3);
        }

        @media (max-width: 968px) {
            .main-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
<div class="container">
  <header style="text-align:center;margin-bottom:18px">
    <div class="brand-logo">ARBY AUDIO</div>
    <div class="small">7.1.4 physics-based spatial audio ‚Äî with multi-object CV mapping (in-browser)</div>
  </header>

  <div class="panel">
    <h3>üìÅ Audio Input</h3>
    <div id="uploadZone" class="upload-zone">Drop audio file here or click ‚Äî WAV/MP3/FLAC/OGG/M4A</div>
    <input id="fileInput" type="file" accept="audio/*" style="display:none"/>
    <div id="fileInfo" style="display:none;margin-top:10px" class="small"></div>
  </div>

  <div class="panel">
    <h3>üé• Video Input</h3>
    <div id="videoUploadZone" class="upload-zone">Drop video file here or click ‚Äî MP4/WEBM/AVI</div>
    <input id="videoInput" type="file" accept="video/*" style="display:none"/>
    <video id="videoPreview" controls style="width:100%;max-height:360px;display:none;margin-top:10px"></video>
    <canvas id="cvCanvas" style="display:none;margin-top:10px"></canvas>
    <div class="small" style="margin-top:8px">Visual overlay: bounding boxes + track IDs (no red dot)</div>
  </div>

  <div class="panel">
    <h3>‚öôÔ∏è Room & Processing</h3>
    <div style="display:flex;gap:10px;flex-wrap:wrap">
      <label>Width <input id="roomWidth" type="number" step="0.1" value="8" min="2" max="20"/></label>
      <label>Length <input id="roomLength" type="number" step="0.1" value="6" min="2" max="25"/></label>
      <label>Height <input id="roomHeight" type="number" step="0.05" value="3.2" min="2" max="12"/></label>
      <label>Reflectivity <input id="wallReflectivity" type="number" step="0.01" value="0.7" min="0.1" max="0.95"/></label>
      <label>AirAbs <input id="airAbsorption" type="number" step="0.001" value="0.01" min="0.001" max="0.05"/></label>
    </div>

    <div class="controls" style="margin-top:12px">
      <button id="analyzeVideoBtn" disabled>üîé Analyze Video (create motion timeline)</button>
      <button id="processBtn" disabled>‚ö° Process Audio (use timeline)</button>
      <button id="playBtn" disabled>‚ñ∂ Play Processed</button>
      <button id="downloadBtn" disabled>üíæ Download WAV</button>
    </div>

    <div class="progress" style="margin-top:10px">
      <div id="globalProgress" class="progress-bar"></div>
    </div>

    <div class="status" id="statusBox">
      <div>Status: <span id="statusText">Ready</span></div>
      <div id="analysisInfo" style="margin-top:6px"></div>
    </div>
  </div>

  <div class="panel">
    <h3>üîä 7.1.4 Speaker Map</h3>
    <div class="speaker-map">
      <div class="speaker">FL</div><div class="speaker">FR</div><div class="speaker">C</div><div class="speaker">LFE</div>
      <div class="speaker">SL</div><div class="speaker">SR</div><div class="speaker">RL</div><div class="speaker">RR</div>
      <div class="speaker">FHL</div><div class="speaker">FHR</div><div class="speaker">RHL</div><div class="speaker">RHR</div>
    </div>
  </div>
</div>

<script>
/*
  Full in-browser merge:
  - motion detection by frame-diff (no external libs)
  - connected-component bounding boxes (floodfill)
  - simple tracker (centroid nearest-match)
  - timeline (per video-sampled-frame) => centroids per frame
  - audio engine uses timeline to spatialize multiple sources per-chunk
  - export 12ch WAV (32-bit float)
*/

/* ---------- Utilities ---------- */
function clamp(v, a, b) { return Math.max(a, Math.min(b, v)); }
function nowMs(){ return performance.now(); }

/* ---------- DOM ---------- */
const uploadZone = document.getElementById('uploadZone');
const fileInput = document.getElementById('fileInput');
const videoUploadZone = document.getElementById('videoUploadZone');
const videoInput = document.getElementById('videoInput');
const videoPreview = document.getElementById('videoPreview');
const cvCanvas = document.getElementById('cvCanvas');
const analyzeVideoBtn = document.getElementById('analyzeVideoBtn');
const processBtn = document.getElementById('processBtn');
const playBtn = document.getElementById('playBtn');
const downloadBtn = document.getElementById('downloadBtn');
const globalProgress = document.getElementById('globalProgress');
const statusText = document.getElementById('statusText');
const analysisInfo = document.getElementById('analysisInfo');

let audioFile = null, videoFile = null;
let timeline = null; // [{t:sec, detections:[{x,y,w,h,centroid:[cx,cy],id}]}, ...]
let videoFPS = 15;   // sampling frames per second for analysis (configurable)
let audioDecodedBuffer = null;
let processedAudioBuffer = null;
let audioContext = null;
let playbackSource = null;

/* ---------- Drag & click hookups ---------- */
uploadZone.addEventListener('click', ()=>fileInput.click());
videoUploadZone.addEventListener('click', ()=>videoInput.click());
fileInput.addEventListener('change', (e)=>{ audioFile = e.target.files[0]; onAudioLoaded(); });
videoInput.addEventListener('change', (e)=>{ videoFile = e.target.files[0]; onVideoLoaded(); });

function setStatus(s){ statusText.textContent = s; }
function setProgress(p){ globalProgress.style.width = clamp(p,0,100) + '%'; }

/* ---------- When audio chosen: decode into AudioBuffer ---------- */
async function onAudioLoaded(){
  setStatus('Decoding audio...');
  if(!audioContext) audioContext = new (window.AudioContext||window.webkitAudioContext)({sampleRate:96000});
  const ab = await audioFile.arrayBuffer();
  audioDecodedBuffer = await audioContext.decodeAudioData(ab.slice(0));
  document.getElementById('fileInfo').style.display='block';
  document.getElementById('fileInfo').textContent = `Audio: ${audioFile.name} ‚Äî ${audioDecodedBuffer.duration.toFixed(2)}s @ ${audioDecodedBuffer.sampleRate}Hz, ${audioDecodedBuffer.numberOfChannels}ch`;
  setStatus('Audio loaded');
  processBtn.disabled = false;
  if(videoFile) analyzeVideoBtn.disabled = false;
}

/* ---------- When video chosen: preview available ---------- */
function onVideoLoaded(){
  videoPreview.src = URL.createObjectURL(videoFile);
  videoPreview.style.display = 'block';
  videoPreview.load();
  analyzeVideoBtn.disabled = false;
  setStatus('Video loaded (preview)');
}

/* ---------- CV Helpers ---------- */
function toGray(imageData){
  const inData = imageData.data;
  const w = imageData.width, h = imageData.height;
  const out = new Uint8ClampedArray(w*h);
  for(let i=0, j=0;i<inData.length;i+=4,j++){
    // luma approximation
    out[j] = (0.2126*inData[i] + 0.7152*inData[i+1] + 0.0722*inData[i+2])|0;
  }
  return {data:out, width:w, height:h};
}

function diffGray(a,b){
  const n = a.data.length;
  const out = new Uint8ClampedArray(n);
  for(let i=0;i<n;i++){
    out[i] = Math.abs(a.data[i] - b.data[i]);
  }
  return {data:out, width:a.width, height:a.height};
}

function thresholdGray(g,th){
  const out = new Uint8ClampedArray(g.data.length);
  for(let i=0;i<g.data.length;i++) out[i] = g.data[i] > th ? 255 : 0;
  return {data:out, width:g.width, height:g.height};
}

/* Simple dilation (box 3x3) to connect regions */
function dilate(bin){
  const w=bin.width,h=bin.height;
  const inD = bin.data;
  const out = new Uint8ClampedArray(inD.length);
  for(let y=0;y<h;y++){
    for(let x=0;x<w;x++){
      let idx = y*w+x;
      if(inD[idx]===255){ out[idx]=255; continue; }
      // check neighbors
      let found=false;
      for(let oy=-1;oy<=1 && !found;oy++){
        for(let ox=-1;ox<=1 && !found;ox++){
          const nx=x+ox, ny=y+oy;
          if(nx<0||nx>=w||ny<0||ny>=h) continue;
          if(inD[ny*w+nx]===255) found=true;
        }
      }
      if(found) out[idx]=255;
    }
  }
  return {data:out,width:w,height:h};
}

/* Connected components (4-neighbor floodfill) ‚Äî returns bounding boxes and centroids */
function connectedComponents(bin, minArea=80){
  const w=bin.width,h=bin.height, data=bin.data;
  const visited = new Uint8Array(data.length);
  const comps = [];
  const stack = [];
  for(let i=0;i<data.length;i++){
    if(data[i]===0 || visited[i]) continue;
    // flood
    let minx=1e9,miny=1e9,maxx=-1,maxy=-1,sumx=0,sumy=0,count=0;
    stack.length=0;
    const start = i;
    stack.push(start);
    visited[start]=1;
    while(stack.length){
      const idx = stack.pop();
      const y = (idx / w)|0;
      const x = idx - y*w;
      minx = Math.min(minx, x); maxx = Math.max(maxx,x);
      miny = Math.min(miny, y); maxy = Math.max(maxy,y);
      sumx += x; sumy += y; count++;
      // neighbors
      const n1 = idx-1, n2 = idx+1, n3 = idx-w, n4 = idx+w;
      if(x>0 && !visited[n1] && data[n1]===255){ visited[n1]=1; stack.push(n1); }
      if(x<w-1 && !visited[n2] && data[n2]===255){ visited[n2]=1; stack.push(n2); }
      if(y>0 && !visited[n3] && data[n3]===255){ visited[n3]=1; stack.push(n3); }
      if(y<h-1 && !visited[n4] && data[n4]===255){ visited[n4]=1; stack.push(n4); }
    }
    if(count >= minArea){
      const cx = sumx / count, cy = sumy / count;
      comps.push({x:minx, y:miny, w:(maxx-minx+1), h:(maxy-miny+1), area:count, centroid:[cx,cy]});
    }
  }
  return comps;
}

/* Simple tracker: match detections to active tracks by nearest centroid */
class Tracker {
  constructor(maxAge=10, maxDistance=80){
    this.tracks = []; // {id, centroid, age, lastFrame}
    this.nextId = 1;
    this.maxAge = maxAge;
    this.maxDistance = maxDistance;
  }
  update(detections, frameIndex){
    const assigned = new Array(detections.length).fill(false);
    // match existing tracks
    for(let t of this.tracks){
      let best=null, bestIdx=-1, bestDist=1e9;
      for(let i=0;i<detections.length;i++){
        if(assigned[i]) continue;
        const dx = detections[i].centroid[0]-t.centroid[0];
        const dy = detections[i].centroid[1]-t.centroid[1];
        const d = Math.hypot(dx,dy);
        if(d < bestDist){ bestDist=d; bestIdx=i; }
      }
      if(bestIdx>=0 && bestDist <= this.maxDistance){
        // update track
        t.centroid = detections[bestIdx].centroid.slice();
        t.bbox = {x:detections[bestIdx].x, y:detections[bestIdx].y, w:detections[bestIdx].w, h:detections[bestIdx].h};
        t.age = 0;
        t.lastFrame = frameIndex;
        assigned[bestIdx]=true;
      } else {
        t.age++;
      }
    }
    // create new tracks for unmatched detections
    for(let i=0;i<detections.length;i++){
      if(assigned[i]) continue;
      const d = detections[i];
      this.tracks.push({id:this.nextId++, centroid:d.centroid.slice(), bbox:{x:d.x,y:d.y,w:d.w,h:d.h}, age:0, lastFrame:frameIndex});
    }
    // remove old tracks
    this.tracks = this.tracks.filter(t=>t.age <= this.maxAge);
    // return snapshot of current tracks
    return this.tracks.map(t=>({id:t.id, centroid:t.centroid.slice(), bbox:t.bbox, lastFrame:t.lastFrame}));
  }
}

/* ---------- Video analysis pipeline ---------- */
async function analyzeVideoToTimeline(videoFileObj, sampleFPS=15, downscale=0.5){
  // returns timeline: array of {t:sec, detections:[{x,y,w,h,centroid}], tracks:[{id,centroid,bbox}]}
  setStatus('Analyzing video for motion timeline');
  setProgress(0);
  const videoURL = URL.createObjectURL(videoFileObj);
  const v = document.createElement('video');
  v.src = videoURL;
  v.muted = true;
  v.preload = 'auto';
  await v.play().catch(()=>{}); // ensure metadata loaded
  await new Promise(r => v.addEventListener('loadedmetadata', r, {once:true}));
  const duration = v.duration;
  const fps = sampleFPS;
  videoFPS = fps;
  const step = 1 / fps;
  const w0 = Math.max(160, Math.floor(v.videoWidth * downscale));
  const h0 = Math.max(120, Math.floor(v.videoHeight * downscale));
  cvCanvas.width = w0; cvCanvas.height = h0; cvCanvas.style.display='block';
  const ctx = cvCanvas.getContext('2d', {willReadFrequently:true});
  const timelineLocal = [];
  const tracker = new Tracker(8, Math.max(20, Math.min(w0,h0)*0.25));

  // prepare previous frame
  let prevGray = null;
  // loop through frames by seeking
  const frames = Math.ceil(duration * fps);
  for(let fi=0; fi<frames; fi++){
    const t = Math.min(duration, fi * step);
    await new Promise(resolve => {
      const onSeek = () => { v.removeEventListener('seeked', onSeek); resolve(); };
      v.addEventListener('seeked', onSeek);
      v.currentTime = t;
    });
    // draw scaled frame
    ctx.drawImage(v, 0, 0, w0, h0);
    const idata = ctx.getImageData(0,0,w0,h0);
    const gray = toGray(idata);
    if(prevGray){
      const d = diffGray(gray, prevGray);
      const thr = thresholdGray(d, 24);
      const dil = dilate(thr);
      const comps = connectedComponents(dil, Math.max(40, (w0*h0*0.0005)|0));
      // map centroid coordinates to original video pixel coordinates ratio
      // here use scaled coords ‚Äî keep consistent
      const detections = comps.map(c=>({
        x:c.x, y:c.y, w:c.w, h:c.h, centroid:c.centroid
      }));
      const tracksNow = tracker.update(detections, fi);
      timelineLocal.push({t: t, detections, tracks: tracksNow});
    } else {
      timelineLocal.push({t: t, detections: [], tracks: []});
    }
    prevGray = gray;
    // overlay visualization for user
    renderOverlay(ctx, videoPreview, timelineLocal[timelineLocal.length-1]);
    setProgress(Math.round((fi/frames)*60)); // analysis uses 0-60% of progress
  }
  v.pause();
  v.src = "";
  URL.revokeObjectURL(videoURL);
  setProgress(65);
  setStatus('Video analysis complete');
  analysisInfo.textContent = `Frames sampled: ${timelineLocal.length} @ ${fps}fps ‚Äî last frame time ${(timelineLocal[timelineLocal.length-1]?.t||0).toFixed(2)}s`;
  return timelineLocal;
}

/* Draw overlay: bounding boxes + track IDs onto provided canvas context */
function renderOverlay(ctx, videoEl, frameInfo){
  // assume latest frame already drawn in canvas; overlay boxes
  if(!frameInfo) return;
  ctx.save();
  ctx.strokeStyle = 'rgba(46,204,113,0.95)';
  ctx.lineWidth = 2;
  ctx.font = '12px monospace';
  ctx.fillStyle = 'rgba(46,204,113,0.95)';
  // draw detections
  for(let d of frameInfo.detections){
    ctx.strokeRect(d.x, d.y, d.w, d.h);
    const cx = Math.round(d.centroid[0]), cy = Math.round(d.centroid[1]);
    ctx.fillText(`o (${cx},${cy})`, d.x+4, d.y+12);
  }
  // draw tracks
  for(let t of frameInfo.tracks){
    ctx.strokeStyle = 'rgba(243,156,18,0.95)';
    ctx.lineWidth = 2;
    ctx.strokeRect(t.bbox.x, t.bbox.y, t.bbox.w, t.bbox.h);
    ctx.fillStyle='rgba(243,156,18,0.98)';
    ctx.fillText(`#${t.id}`, t.bbox.x+4, t.bbox.y+14);
  }
  ctx.restore();
}

/* ---------- Audio Engine (browser version) ---------- */
/* This engine closely mirrors your earlier algorithm but is implemented in JS:
   - produces a 12-channel AudioBuffer
   - accepts a 'timeline' with tracks per sampled video frame
   - maps timeline to audio chunks and renders multiple moving sources
   - exports 32-bit float WAV
*/
class ArbyEngine {
  constructor(ctx){
    this.ctx = ctx;
    this.sampleRate = 96000;
    this.numChannels = 12;
    this.c = 343;
    this.speakerPositions = [
      {name:'FL', az:-45, el:0},{name:'FR', az:45, el:0},{name:'C', az:0, el:0},{name:'LFE',az:0,el:-90},
      {name:'SL',az:-110,el:0},{name:'SR',az:110,el:0},{name:'RL',az:-150,el:0},{name:'RR',az:150,el:0},
      {name:'FHL',az:-45,el:45},{name:'FHR',az:45,el:45},{name:'RHL',az:-150,el:45},{name:'RHR',az:150,el:45}
    ];
  }

  calculateSpeakerGainsRad(az, el){
    // inputs in radians
    const gains = new Float32Array(this.numChannels);
    // simple cosine windows as previously
    const cos = Math.cos, sin = Math.sin;
    const azDeg = az * 180/Math.PI, elDeg = el * 180/Math.PI;
    for(let i=0;i<this.speakerPositions.length;i++){
      const sp = this.speakerPositions[i];
      const da = Math.abs(azDeg - sp.az);
      const de = Math.abs(elDeg - sp.el);
      const angularDist = Math.sqrt(da*da + de*de);
      gains[i] = 1 / (1 + angularDist/45);
    }
    // normalize
    let sum=0; for(let g of gains) sum += g*g;
    const norm = sum>0? 1/Math.sqrt(sum):1;
    for(let i=0;i<gains.length;i++) gains[i]*=norm;
    return gains;
  }

  reflectPoint(point, planePoint, planeNormal){
    const d = ((point[0]-planePoint[0])*planeNormal[0] + (point[1]-planePoint[1])*planeNormal[1] + (point[2]-planePoint[2])*planeNormal[2]);
    return [ point[0]-2*d*planeNormal[0], point[1]-2*d*planeNormal[1], point[2]-2*d*planeNormal[2] ];
  }

  async processWithTimeline(monoFloat32, audioSampleRate, config, timeline, videoFPS, onProgress){
    // monoFloat32 = Float32Array mono data at audioSampleRate
    // We'll upsample linearly to this.sampleRate then chunk-process, mapping chunk time to timeline frame index
    setStatus('Rendering spatial audio (this runs in the browser ‚Äî CPU heavy)');
    onProgress(0);
    // Upsample if needed
    let audio = monoFloat32;
    if(audioSampleRate !== this.sampleRate){
      const ratio = this.sampleRate / audioSampleRate;
      const newLen = Math.floor(audio.length * ratio);
      const up = new Float32Array(newLen);
      for(let i=0;i<newLen;i++){
        const srcIdx = i/ratio;
        const f = Math.floor(srcIdx), c = Math.min(f+1, audio.length-1);
        const frac = srcIdx - f;
        up[i] = audio[f]*(1-frac) + audio[c]*frac;
      }
      audio = up;
    }
    onProgress(5);

    const numSamples = audio.length;
    const out = Array.from({length:this.numChannels}, ()=>new Float32Array(numSamples));
    const chunkSize = 4096;
    const numChunks = Math.ceil(numSamples / chunkSize);

    // prepare reflection walls
    const walls = [
      {name:'front', normal:[0,0,1], pos:[0,0,config.roomLength/2]},
      {name:'back', normal:[0,0,-1], pos:[0,0,-config.roomLength/2]},
      {name:'left', normal:[1,0,0], pos:[-config.roomWidth/2,0,0]},
      {name:'right', normal:[-1,0,0], pos:[config.roomWidth/2,0,0]},
      {name:'ceiling', normal:[0,-1,0], pos:[0,config.roomHeight/2,0]},
      {name:'floor', normal:[0,1,0], pos:[0,-config.roomHeight/2,0]}
    ];

    const listener = [0,0,0];
    let currentReflGain = config.wallReflectivity;

    // For each chunk, map to timeline frame index(s) ‚Äî timeline sampled at videoFPS
    for(let chunk=0; chunk<numChunks; chunk++){
      const start = chunk*chunkSize;
      const end = Math.min(start+chunkSize, numSamples);
      const tStart = start / this.sampleRate;
      const frameIdx = Math.min(timeline.length-1, Math.floor(tStart * videoFPS));
      const frameData = timeline[Math.max(0, frameIdx)] || {tracks:[]};

      // if multiple tracks, each is a source
      const sources = [];
      // map each track centroid (in canvas coordinates scaled to video width/height) to room coords
      // for simplicity: assume timeline centroids are in canvas dims and we treat canvas width->roomWidth, canvas height->roomLength
      // We recorded centroids in CV canvas scaled to videoPreview dims when analyzing; we saved as scaled coords already.
      for(const tr of frameData.tracks){
        // centroid expected as [cx,cy] in canvas pixel coords; convert to room coords:
        const cvs = cvCanvas;
        const cx = tr.centroid[0], cy = tr.centroid[1];
        const rx = (cx / cvs.width) * config.roomWidth - config.roomWidth/2;
        const ry = (cy / cvs.height) * config.roomLength - config.roomLength/2;
        const rz = 0.15; // fixed small height
        sources.push({pos:[rx, ry, rz], id:tr.id});
      }
      // if no sources detected, fall back to a default moving source in timeline fallback
      if(sources.length===0){
        const defaultPos = [2 * Math.sin(tStart*0.5), 0, 0.15];
        sources.push({pos: defaultPos, id: -1});
      }

      // For each source, add its direct contribution and reflections into out buffer between start..end
      for(const src of sources){
        const sPos = src.pos;
        // direct distance
        const dx = listener[0]-sPos[0], dy = listener[1]-sPos[1], dz = listener[2]-sPos[2];
        const dDirect = Math.hypot(dx,dy,dz) + 1e-9;
        const directDelay = Math.floor((dDirect / this.c) * this.sampleRate);
        const k_base = config.enableLinearScaling ? (0.3 + ((config.roomWidth+config.roomLength+config.roomHeight)/3)/15*0.4) : 0.5;
        const p_exp = config.enableLinearScaling ? (1.0 + ((config.roomWidth+config.roomLength+config.roomHeight)/3)/15*0.5) : 1.2;
        const directGain = 1 / (1 + k_base * Math.pow(dDirect, p_exp));
        const vec = [listener[0]-sPos[0], listener[1]-sPos[1], listener[2]-sPos[2]];
        const vecNorm = dDirect > 1e-9 ? [vec[0]/dDirect, vec[1]/dDirect, vec[2]/dDirect] : [1,0,0];
        const az = Math.atan2(vecNorm[1], vecNorm[0]);
        const el = Math.asin(clamp(vecNorm[2], -1, 1));
        const directGains = this.calculateSpeakerGainsRad(az, el);

        // add direct (with delay) ‚Äî e.g., out[ch][i] += audio[i - directDelay]*directGain*directGains[ch]
        for(let i = start; i < end; i++){
          const sidx = i - directDelay;
          if(sidx >= 0 && sidx < audio.length){
            const sample = audio[sidx] * directGain;
            for(let ch=0; ch<this.numChannels; ch++){
              out[ch][i] += sample * directGains[ch];
            }
          }
        }

        // reflections per wall
        for(const wall of walls){
          const s_ref = this.reflectPoint(sPos, wall.pos, wall.normal);
          const dxr = listener[0]-s_ref[0], dyr = listener[1]-s_ref[1], dzr = listener[2]-s_ref[2];
          const dRef = Math.hypot(dxr,dyr,dzr) + 1e-9;
          const refDelay = Math.floor((dRef / this.c) * this.sampleRate);
          const distAtt = 1 / (1 + k_base * Math.pow(dRef, p_exp));
          const airAbs = Math.exp(-config.airAbsorption * dRef);
          const Rw = currentReflGain * distAtt * airAbs;
          // lowpass cutoff roughly per distance
          const maxRoomDist = Math.hypot(config.roomWidth, config.roomLength, config.roomHeight);
          const normDist = Math.min(dRef / maxRoomDist, 1);
          const cutoff = this.sampleRate * (0.45 - 0.35 * normDist);
          // approximate LP by naive one-sample smoothing per sample (we already have heavy CPU in JS)
          let prevOutVal = 0;
          const alpha = 1 / (1 + (this.sampleRate / Math.max(1, cutoff)));
          const vecRef = [listener[0]-s_ref[0], listener[1]-s_ref[1], listener[2]-s_ref[2]];
          const dRefNorm = dRef;
          const azr = Math.atan2(vecRef[1]/dRefNorm, vecRef[0]/dRefNorm);
          const elr = Math.asin(clamp(vecRef[2]/dRefNorm, -1,1));
          const refGains = this.calculateSpeakerGainsRad(azr, elr);
          for(let i = start; i < end; i++){
            const sidx = i - refDelay;
            if(sidx >= 0 && sidx < audio.length){
              // very small LP: simple exponential smoothing applied to sample
              const raw = audio[sidx];
              prevOutVal = prevOutVal + alpha * (raw - prevOutVal);
              const sample = prevOutVal * Rw;
              for(let ch=0; ch<this.numChannels; ch++){
                out[ch][i] += sample * refGains[ch];
              }
            }
          }
        } // walls loop
      } // each source

      // adaptive reflection gain (coarse)
      if(config.enableAdaptiveGain){
        // compute chunk peak
        let chunkPeak = 0;
        for(let ch=0; ch<this.numChannels; ch++){
          for(let i=start;i<end;i++) chunkPeak = Math.max(chunkPeak, Math.abs(out[ch][i]));
        }
        if(chunkPeak > 0.8) currentReflGain *= 0.95;
        else if(chunkPeak < 0.5 && currentReflGain < config.wallReflectivity) currentReflGain *= 1.02;
      }

      if(chunk % Math.max(1, Math.floor(numChunks/40)) === 0){
        onProgress(5 + Math.floor((chunk/numChunks)*85)); // use 5-90%
      }
    } // chunk loop

    // normalize full buffer
    let maxSample = 0;
    for(let ch=0;ch<this.numChannels;ch++){
      for(let i=0;i<numSamples;i++) maxSample = Math.max(maxSample, Math.abs(out[ch][i]));
    }
    if(maxSample > 0.95){
      const norm = 0.95 / maxSample;
      for(let ch=0;ch<this.numChannels;ch++){
        for(let i=0;i<numSamples;i++) out[ch][i] *= norm;
      }
    }

    // assemble AudioBuffer
    const buf = this.ctx.createBuffer(this.numChannels, audio.length, this.sampleRate);
    for(let ch=0; ch<this.numChannels; ch++){
      buf.copyToChannel(out[ch], ch, 0);
    }
    onProgress(100);
    setStatus('Rendering complete');
    return buf;
  }

  exportWAVBuffer(audioBuffer){
    // return Blob (32-bit float WAV) for multi-channel buffer
    const numChannels = audioBuffer.numberOfChannels;
    const numFrames = audioBuffer.length;
    const sampleRate = audioBuffer.sampleRate;
    // 32-bit float PCM (WAVE_FORMAT_IEEE_FLOAT)
    const bytesPerSample = 4;
    const blockAlign = numChannels * bytesPerSample;
    const dataSize = numFrames * blockAlign;
    const buffer = new ArrayBuffer(44 + dataSize);
    const view = new DataView(buffer);
    // RIFF header
    writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + dataSize, true);
    writeString(view, 8, 'WAVE');
    // fmt chunk
    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true); // chunk size
    view.setUint16(20, 3, true); // format = 3 (IEEE float)
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bytesPerSample * 8, true);
    // data chunk
    writeString(view, 36, 'data');
    view.setUint32(40, dataSize, true);
    // write samples interleaved
    let offset = 44;
    const channelData = [];
    for(let ch=0; ch<numChannels; ch++) channelData.push(audioBuffer.getChannelData(ch));
    for(let i=0;i<numFrames;i++){
      for(let ch=0;ch<numChannels;ch++){
        view.setFloat32(offset, channelData[ch][i], true);
        offset += 4;
      }
    }
    return new Blob([view], {type: 'audio/wav'});
  }
}

/* ---------- helper for WAV writing ---------- */
function writeString(view, offset, str){
  for(let i=0;i<str.length;i++) view.setUint8(offset+i, str.charCodeAt(i));
}

/* ---------- UI Actions: analyze video => process audio using timeline ---------- */
analyzeVideoBtn.addEventListener('click', async ()=>{
  if(!videoFile){ alert('Choose a video first'); return; }
  analyzeVideoBtn.disabled = true;
  setStatus('Analyzing video...');
  timeline = await analyzeVideoToTimeline(videoFile, videoFPS, 0.5);
  setProgress(70);
  analyzeVideoBtn.disabled = false;
  processBtn.disabled = false;
});

processBtn.addEventListener('click', async ()=>{
  if(!audioDecodedBuffer){ alert('Load audio first'); return; }
  // build mono Float32 array from audioDecodedBuffer
  const nch = audioDecodedBuffer.numberOfChannels;
  const numFrames = audioDecodedBuffer.length;
  const mono = new Float32Array(numFrames);
  for(let ch=0; ch<nch; ch++){
    const d = audioDecodedBuffer.getChannelData(ch);
    for(let i=0;i<numFrames;i++) mono[i] += d[i] / nch;
  }
  // config
  <input type="file" id="videoInput">
<input type="file" id="audioInput">
<button id="processBtn">Process Audio</button>
<button id="mergeBtn" disabled>Merge & Download</button>
<button id="playBtn" disabled>Play</button>
<button id="downloadBtn" disabled>Download</button>
<script type="module">
import { createFFmpeg, fetchFile } from 'https://unpkg.com/@ffmpeg/ffmpeg@0.12.0/dist/ffmpeg.mjs';

const ffmpeg = createFFmpeg({ log: true });
const videoInput = document.getElementById('videoInput');
const audioInput = document.getElementById('audioInput');
const processBtn = document.getElementById('processBtn');
const mergeBtn = document.getElementById('mergeBtn');
const playBtn = document.getElementById('playBtn');
const downloadBtn = document.getElementById('downloadBtn');

let processedAudioBlob = null;
let audioContext = new (window.AudioContext || window.webkitAudioContext)();

function interleaveChannels(channels) {
  const N = channels[0].length;
  const numCh = channels.length;
  const out = new Float32Array(N * numCh);
  for(let i=0;i<N;i++){
    for(let ch=0; ch<numCh; ch++){
      out[i*numCh + ch] = channels[ch][i];
    }
  }
  return out;
}

function encodeWAV(channels, sampleRate=96000){
  const numCh = channels.length;
  const N = channels[0].length;
  const buffer = new ArrayBuffer(44 + N*4*numCh);
  const view = new DataView(buffer);
  let offset = 0;
  function writeStr(s){ for(let i=0;i<s.length;i++) view.setUint8(offset++, s.charCodeAt(i)); }
  function write32(v){ view.setUint32(offset, v, true); offset+=4; }
  function write16(v){ view.setUint16(offset, v, true); offset+=2; }

  writeStr('RIFF'); write32(36 + N*4*numCh); writeStr('WAVE');
  writeStr('fmt '); write32(16); write16(3); write16(numCh); write32(sampleRate); write32(sampleRate*numCh*4);
  write16(numCh*4); write16(32);
  writeStr('data'); write32(N*4*numCh);

  const interleaved = interleaveChannels(channels);
  for(let i=0;i<interleaved.length;i++) view.setFloat32(offset, interleaved[i], true), offset+=4;

  return new Blob([buffer], {type:'audio/wav'});
}

// Dummy functions for status/progress (replace with your own UI handlers)
function setStatus(msg){ console.log(msg); }
function setProgress(p){ console.log('Progress:', p); }

processBtn.onclick = async () => {
  const audioFile = audioInput.files[0];
  if(!audioFile) return alert('Select an audio file!');
  processBtn.disabled = true;
  setStatus('Decoding audio...');
  const arrayBuffer = await audioFile.arrayBuffer();
  const audioDecodedBuffer = await audioContext.decodeAudioData(arrayBuffer);

  const config = {
    roomWidth: 5, roomLength: 5, roomHeight: 3,
    wallReflectivity: 0.5, airAbsorption: 0.1,
    enableAdaptiveGain: true, enableLinearScaling: true,
    enableAutoRoomScale: false
  };

  let timeline = null; // your timeline logic here if needed
  if(!timeline){
    timeline = [];
    const frames = Math.ceil(audioDecodedBuffer.duration * 15);
    for(let i=0;i<frames;i++) timeline.push({t:i/15, detections:[], tracks:[]});
  }

  setStatus('Processing audio (spatial render)...');
  const engine = new ArbyEngine(audioContext); // your ArbyEngine
  const renderedBuf = await engine.processWithTimeline(
    [audioDecodedBuffer.getChannelData(0)], audioDecodedBuffer.sampleRate,
    config, timeline, 15, setProgress
  );

  processedAudioBlob = encodeWAV(renderedBuf, audioDecodedBuffer.sampleRate);
  setStatus('Processed ‚Äî ready to play/download');
  playBtn.disabled = false;
  downloadBtn.disabled = false;
  mergeBtn.disabled = false;
  processBtn.disabled = false;
};

mergeBtn.onclick = async () => {
  const videoFile = videoInput.files[0];
  if (!videoFile || !processedAudioBlob) return alert('Select video and process audio first!');

  if (!ffmpeg.isLoaded()) await ffmpeg.load();

  ffmpeg.FS('writeFile', 'video.mp4', await fetchFile(videoFile));
  ffmpeg.FS('writeFile', 'audio.wav', await fetchFile(processedAudioBlob));

  await ffmpeg.run(
    '-i', 'video.mp4',
    '-i', 'audio.wav',
    '-c:v', 'libx264',
    '-c:a', 'aac',
    '-b:a', '192k',
    '-shortest',
    'merged.mp4'
  );

  const data = ffmpeg.FS('readFile', 'merged.mp4');
  const blob = new Blob([data.buffer], { type: 'video/mp4' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = 'ARBY_merged.mp4';
  a.click();
};

// Optional: play processed audio
playBtn.onclick = () => {
  if(!processedAudioBlob) return;
  const url = URL.createObjectURL(processedAudioBlob);
  const audio = new Audio(url);
  audio.play();
};

// Optional: download processed audio
downloadBtn.onclick = () => {
  if(!processedAudioBlob) return;
  const url = URL.createObjectURL(processedAudioBlob);
  const a = document.createElement('a');
  a.href = url;
  a.download = 'processed_audio.wav';
  a.click();
};



/* play/download */
playBtn.addEventListener('click', async ()=>{
  if(!processedAudioBuffer) return;
  if(!audioContext) audioContext = new (window.AudioContext||window.webkitAudioContext)({sampleRate:96000});
  if(playbackSource){ playbackSource.stop(); playbackSource.disconnect(); playbackSource=null; }
  playbackSource = audioContext.createBufferSource();
  playbackSource.buffer = processedAudioBuffer;
  // if browser cannot handle multichannel output to device, it will downmix to stereo automatically
  playbackSource.connect(audioContext.destination);
  playbackSource.start();
});

downloadBtn.addEventListener('click', ()=>{
  if(!processedAudioBuffer){ alert('Process audio first'); return; }
  const eng = new ArbyEngine(audioContext);
  const blob = eng.exportWAVBuffer(processedAudioBuffer);
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url; a.download = 'arby_processed_7_1_4_96k_32f.wav';
  document.body.appendChild(a); a.click(); a.remove();
});

/* ---------- small UX: draw real-time overlay while video preview plays (optional) ---------- */
videoPreview.addEventListener('play', function(){
  cvCanvas.width = videoPreview.videoWidth || cvCanvas.width;
  cvCanvas.height = videoPreview.videoHeight || cvCanvas.height;
  cvCanvas.style.display='block';
  const ctx = cvCanvas.getContext('2d');
  let prev = null;
  const trackerLocal = new Tracker(8, Math.max(20, Math.min(cvCanvas.width, cvCanvas.height)*0.25));
  function step(){
    if(videoPreview.paused || videoPreview.ended) return;
    ctx.drawImage(videoPreview, 0, 0, cvCanvas.width, cvCanvas.height);
    const idata = ctx.getImageData(0,0,cvCanvas.width, cvCanvas.height);
    const gray = toGray(idata);
    if(prev){
      const d = diffGray(gray, prev);
      const thr = thresholdGray(d, 24);
      const dil = dilate(thr);
      const comps = connectedComponents(dil, Math.max(40, (cvCanvas.width*cvCanvas.height*0.0005)|0));
      const detections = comps.map(c=>({x:c.x,y:c.y,w:c.w,h:c.h,centroid:c.centroid}));
      const tracks = trackerLocal.update(detections, Date.now());
      // re-draw frame for overlay
      ctx.drawImage(videoPreview, 0, 0, cvCanvas.width, cvCanvas.height);
      // overlay
      for(const d of detections){
        ctx.strokeStyle='rgba(46,204,113,0.9)'; ctx.lineWidth=2; ctx.strokeRect(d.x,d.y,d.w,d.h);
        ctx.fillStyle='rgba(46,204,113,0.95)'; ctx.fillText(`o`, d.x+4, d.y+12);
      }
      for(const t of tracks){
        ctx.strokeStyle='rgba(243,156,18,0.95)'; ctx.lineWidth=2;
        ctx.strokeRect(t.bbox.x,t.bbox.y,t.bbox.w,t.bbox.h);
        ctx.fillStyle='rgba(243,156,18,0.98)'; ctx.fillText(`#${t.id}`, t.bbox.x+4, t.bbox.y+14);
      }
    }
    prev = gray;
    requestAnimationFrame(step);
  }
  step();
});

setStatus('Ready');

</script>
</body>
</html>
